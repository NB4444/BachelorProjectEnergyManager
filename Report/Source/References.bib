@misc{Russel,
author = {Russel, Daniel M.},
mendeley-groups = {Master Project/Selected},
title = {{Google Advanced Search Operators}},
url = {https://docs.google.com/document/d/1ydVaJJeL1EYbWtlfj9TPfBTE5IBADkQfZrQaBZxqXGs},
urldate = {2020-07-19}
}
@misc{NVIDIA,
author = {NVIDIA},
mendeley-groups = {Master Project/Selected},
title = {{NVIDIA System Management Interface | NVIDIA Developer}},
url = {https://developer.nvidia.com/nvidia-system-management-interface},
urldate = {2020-07-18}
}
@inproceedings{Dakkak2019,
abstract = {Driven by deep learning, there has been a surge of specialized processors for matrix multiplication, referred to as Tensor Core Units (TCUs). These TCUs are capable of performing matrix multiplications on small matrices (usually 4 × 4 or 16 × 16) to accelerate HPC and deep learning workloads. Although TCUs are prevalent and promise increase in performance and/or energy efficiency, they suffer from over specialization as only matrix multiplication on small matrices is supported. In this paper we express both reduction and scan in terms of matrix multiplication operations and map them onto TCUs. To our knowledge, this paper is the first to try to broaden the class of algorithms expressible as TCU operations and is the first to show benefits of this mapping in terms of: program simplicity, efficiency, and performance. We implemented the reduction and scan algorithms using NVIDIA's V100 TCUs and achieved 89{\%} - 98{\%} of peak memory copy bandwidth. Our results are orders of magnitude faster (up to 100 × for reduction and 3 × for scan) than state-of-the-art methods for small segment sizes (common in HPC and deep learning applications). Our implementation achieves this speedup while decreasing the power consumption by up to 22{\%} for reduction and 16{\%} for scan.},
author = {Dakkak, Abdul and Li, Cheng and Xiong, Jinjun and Gelado, Isaac and Hwu, Wen Mei},
booktitle = {Proceedings of the International Conference on Supercomputing},
doi = {10.1145/3330345.3331057},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dakkak et al. - 2019 - Accelerating Reduction and Scan Using Tensor Core Units.pdf:pdf},
isbn = {9781450360791},
mendeley-groups = {Literature Study and Seminar/Selected,Master Project/Selected},
month = {jun},
pages = {46--57},
publisher = {Association for Computing Machinery},
title = {{Accelerating reduction and scan using tensor core units}},
url = {https://doi.org/10.1145/3330345.3331057},
year = {2019}
}
@article{Bal2016,
abstract = {The Dutch Advanced School for Computing and Imaging has built five generations of a 200-node distributed system over nearly two decades while remaining aligned with the shifting computer science research agenda. The system has supported years of award-winning research, underlining the benefits of investing in a smaller-scale, tailored design.},
author = {Bal, Henri and Epema, Dick and {De Laat}, Cees and {Van Nieuwpoort}, Rob and Romein, John and Seinstra, Frank and Snoek, Cees and Wijshoff, Harry},
doi = {10.1109/MC.2016.127},
file = {::},
issn = {00189162},
journal = {Computer},
keywords = {collaborative computing,computer architecture,computer science research,computing for education,distributed architectures,distributed programming,distributed systems,e-science research,experimental environments,heterogeneous computing,homogeneous system organization,network optimization,parallel architectures,research infrastructures},
mendeley-groups = {Master Project/Selected},
number = {5},
pages = {54--63},
title = {{A Medium-Scale Distributed System for Computer Science Research: Infrastructure for the Long Term}},
url = {https://isis-data.science.uva.nl/cgmsnoek/pub/bal-das-computer.pdf},
volume = {49},
year = {2016}
}
@inproceedings{Park2014,
abstract = {Contemporary mobile platforms use mobile GPUs for graphics-intensive applications, and deploy proprietary Dynamic Voltage Frequency Scaling (DVFS) policies in an attempt to save energy without sacrificing quality. However, there have been no previous systematic studies to correlate the performance, power, and energy efficiency of mobile GPUs based on diverse graphics workloads to enable more efficient mobile platform DVFS policies for energy savings. For the first time we present a study of mobile GPU graphics workload characterization for DVFS design considering user experience and energy efficiency on a real smart-phone. We develop micro-benchmarks that stress specific stages of the graphics pipeline separately, and study the relationship between varying graphics workloads and resulting energy and performance of different mobile graphics pipeline stages. We use these results to outline opportunities for more efficient, integrated DVFS policies across the mobile GPU, memory and CPU hardware components for saving energy without sacrificing user experience. Our experimental results on the Nexus 4 smartphone show that it is important to characterize GPU hardware and graphics workloads accurately in order to achieve increased energy efficiency without degradation in graphics performance for better user experience. We believe that our observations and results will enable more energy-efficient DVFS algorithms for mobile graphics rendering in the face of rapidly changing mobile GPU architectures.},
author = {Park, Jurn Gyu and Hsieh, Chen Ying and Dutt, Nikil and Lim, Sung Soo},
booktitle = {2014 IEEE 12th Symposium on Embedded Systems for Real-Time Multimedia, ESTIMedia 2014},
doi = {10.1109/ESTIMedia.2014.6962347},
file = {::},
isbn = {9781479963072},
mendeley-groups = {Master Project/Unread},
month = {nov},
pages = {70--79},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Quality-aware mobile graphics workload characterization for energy-efficient DVFS design}},
year = {2014}
}
@inproceedings{Wang2014,
abstract = {GPUs have been proven very effective for structured applications. However, emerging data intensive applications are increasingly unstructured - irregular in their memory and control flow behavior over massive data sets. While the irregularity in these applications can result in poor workload balance among fine-grained threads or coarse-grained blocks, one can still observe dynamically formed pockets of structured data parallelism that can locally effectively exploit the GPU compute and memory bandwidth. In this study, we seek to characterize such dynamically formed parallelism and and evaluate implementations designed to exploit them using CUDA Dynamic Parallelism (CDP) - an execution model where parallel workload are launched dynamically from within kernels when pockets of structured parallelism are detected. We characterize and evaluate such implementations by analyzing the impact on control and memory behavior measurements on commodity hardware. In particular, the study targets a comprehensive understanding of the overhead of current CDP support in GPUs in terms of kernel launch, memory footprint and algorithm overhead. Experiments show that while the CDP implementation can generate potentially 1.13x-2.73x speedup over non-CDP implementations, the non-trivial overhead causes the overall performance an average of 1.21x slowdown.},
author = {Wang, Jin and Yalamanchili, Sudhakar},
booktitle = {IISWC 2014 - IEEE International Symposium on Workload Characterization},
doi = {10.1109/IISWC.2014.6983039},
file = {::},
isbn = {9781479964536},
mendeley-groups = {Master Project/Unread},
pages = {51--60},
title = {{Characterization and analysis of dynamic parallelism in unstructured GPU applications}},
url = {http://casl.gatech.edu/wp-content/uploads/2014/09/wang{\_}iiswc2014.pdf},
year = {2014}
}
@misc{Mittal2014,
abstract = {Recent years have witnessed phenomenal growth in the computational capabilities and applications of GPUs. However, this trend has also led to a dramatic increase in their power consumption. This article surveys research works on analyzing and improving energy efficiency of GPUs. It also provides a classification of these techniques on the basis of their main research idea. Further, it attempts to synthesize research works that compare the energy efficiency of GPUs with other computing systems (e.g., FPGAs and CPUs). The aim of this survey is to provide researchers with knowledge of the state of the art in GPU power management and motivate them to architect highly energy-efficient GPUs of tomorrow.},
annote = {GPUs seem to be more energy efficient and perform better for applications that are embarassingly parallel.
Next are FPGAs, which seem to be more efficient on both those aspects for all other applications.
CPUs seem to mostly be at the bottom of the list.

Questions:
- When will the project start?},
archivePrefix = {arXiv},
arxivId = {1404.4629},
author = {Mittal, Sparsh and Vetter, Jeffrey S.},
booktitle = {ACM Computing Surveys},
doi = {10.1145/2636342},
eprint = {1404.4629},
file = {::},
issn = {15577341},
keywords = {Architecture techniques,Energy efficiency,Energy saving,GPU (graphics-processing unit),Green computing,Power management,Power model},
mendeley-groups = {Master Project},
month = {apr},
number = {2},
title = {{A survey of methods for analyzing and improving gpu energy efficiency}},
url = {http://arxiv.org/abs/1404.4629},
volume = {47},
year = {2014}
}
@article{LENZI2015,
abstract = {La tesi affronta il problema della modellazione dei consumi energetici in computazioni data parallel (map) in architetture con GPU. I modelli sviluppati sono utilizzati per valutare il compromesso tra performance e consumi. La tesi include risultati sperimentali che validano sia i modelli sviluppati che la metodologia di ricerca di un compromesso tra prestazioni e consumi.},
author = {LENZI, ALESSANDRO},
file = {::},
keywords = {INFORMATICA,Tesi di laurea magistrale,cpu,cuda,data parallel,energy aware computing,gpu,heterogeneous systems,high performance computing,parallel programming},
mendeley-groups = {Master Project},
month = {dec},
title = {{Energy models in data parallel CPU/GPU computations}},
year = {2015}
}
@inproceedings{Li2011,
abstract = {Enterprise workloads like search, data mining and analytics, etc. typically involve a large number of users who are simultaneously using applications that are hosted on clusters of commodity computers. Use of GPUs for enterprise computing is challenging because of poor performance and higher energy consumption compared to running enterprise workloads on CPUs. In this paper, we show that the GPU work consolidation can improve system throughput and results in significant energy savings over multicore CPUs. We develop a novel runtime framework that dynamically consolidates instances from different workloads from multiple user processes into a single GPU workload. However, arbitrary consolidation of GPU workloads does not always lead to better energy efficiency. We use new GPU performance and power models to make predictions for potential workload consolidation alternatives and identify useful consolidations. Our experiments on a variety of workloads (that perform poorly on a GPU compared to well optimized multicore CPU implementations) show that the proposed framework for GPU can provide 2X to 22X energy benefit over a multicore CPU. {\textcopyright} 2011 IEEE.},
author = {Li, Dong and Byna, Surendra and Chakradhar, Srimat},
booktitle = {Proceedings of the International Conference on Parallel Processing Workshops},
doi = {10.1109/ICPPW.2011.25},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Byna, Chakradhar - 2011 - Energy-aware workload consolidation on GPU.pdf:pdf},
isbn = {9780769545110},
issn = {15302016},
keywords = {GPU computing,Power aware computing,Workload consolidation},
mendeley-groups = {Master Project/Selected},
pages = {389--398},
title = {{Energy-aware workload consolidation on GPU}},
url = {https://www.researchgate.net/publication/224263014},
year = {2011}
}
@inproceedings{Hong2010,
abstract = {GPU architectures are increasingly important in the multi-core era due to their high number of parallel processors. Performance optimization for multi-core processors has been a challenge for programmers. Furthermore, optimizing for power consumption is even more difficult. Unfortunately, as a result of the high number of processors, the power consumption of many-core processors such as GPUs has increased significantly. Hence, in this paper, we propose an integrated power and performance (IPP) prediction model for a GPU architecture to predict the optimal number of active processors for a given application. The basic intuition is that when an application reaches the peak memorybandwidth, using more cores does not result in performance improvement. We develop an empirical power model for the GPU. Unlike most previous models, which require measured execution times, hardware performance counters, or architectural simulations, IPP predicts execution times to calculate dynamic power events. We then use the outcome of IPP to control the number of running cores. We also model the increases in power consumption that resulted from the increases in temperature. With the predicted optimal number of active cores, we show that we can save up to 22.09{\%}of runtime GPU energy consumption and on average 10.99{\%} of that for the five memory bandwidth-limited benchmarks. Copyright 2010 ACM.},
author = {Hong, Sunpyo and Kim, Hyesoon},
booktitle = {Proceedings - International Symposium on Computer Architecture},
doi = {10.1145/1815961.1815998},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hong, Kim - 2010 - An integrated GPU power and performance model.pdf:pdf},
isbn = {9781450300520},
issn = {10636897},
keywords = {Analytical model,CUDA,Energy,GPU architecture,Performance,Powerestimation},
mendeley-groups = {Master Project/Selected},
pages = {280--289},
title = {{An integrated GPU power and performance model}},
url = {http://www.cs.binghamton.edu/{~}millerti/cs680r/papers/GPU/AnIntegratedGPU.pdf},
year = {2010}
}
@inproceedings{Nagasaka2010,
abstract = {We present a statistical approach for estimating power consumption of GPU kernels. We use the GPU perfor• mance counters that are exposed for CUDA applications, and train a linear regression model where performance counters are used as independent variables and power consumption is the dependent variable. For model training and evaluation, we use publicly available CUDA applications, consisting of 49 kernels in the CUDA SDK and the Rodinia benchmark suite. Our regression model achieves highly accurate estimates for many of the tested kernels, where the average error ratio is 4.7{\%}. However, we also find that it fails to yield accurate estimates for kernels with texture reads because of the lack of performance counters for monitoring texture accesses, resulting in significant underestimation for such kernels. {\textcopyright}2010 IEEE.},
author = {Nagasaka, Hitoshi and Maruyama, Naoya and Nukada, Akira and Endo, Toshio and Matsuoka, Satoshi},
booktitle = {2010 International Conference on Green Computing, Green Comp 2010},
doi = {10.1109/GREENCOMP.2010.5598315},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagasaka et al. - 2010 - Statistical power modeling of GPU kernels using performance counters(2).pdf:pdf},
isbn = {9781424476138},
mendeley-groups = {Master Project/Selected},
pages = {115--122},
title = {{Statistical power modeling of GPU kernels using performance counters}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.232.4758},
year = {2010}
}
@inproceedings{Chen2011,
abstract = {Graphics Processing Units (GPUs) have emerged as a promising platform for parallel computation. With a large number of scalar processors and abundant memory bandwidth, GPUs provide substantial computation power. While delivering high computation performance, the GPU also consumes high power and needs to be equipped with sufficient power supplies and cooling systems. Therefore, it is essential to institute an efficient mechanism for evaluating and understanding the power consumption requirement when running real applications on high-end GPUs. In this paper, we present a high-level GPU power consumption model using sophisticated tree-based random forest methods which can correlate the power consumption with a set of independent performance variables. This statistical model not only predicts the GPU runtime power consumption accurately, but more importantly, it also provides sufficient insights for understanding the dependence between the GPU runtime power consumption and the individual performance metrics. In order to gain more insights, we use a GPU simulator that can collect more runtime performance metrics than hardware counters. We measure the power consumption of a wide-range of CUDA kernels on an experimental system with GTX 280 GPU as statistical samples for our power analysis. This methodology can certainly be applied to any other CUDA GPU. {\textcopyright} 2011 IEEE.},
author = {Chen, Jianmin and Li, Bin and Zhang, Ying and Peng, Lu and Peir, Jih Kwon},
booktitle = {2011 International Green Computing Conference and Workshops, IGCC 2011},
doi = {10.1109/IGCC.2011.6008582},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2011 - Statistical GPU power analysis using tree-based methods.pdf:pdf},
isbn = {9781457712203},
mendeley-groups = {Master Project/Selected},
title = {{Statistical GPU power analysis using tree-based methods}},
year = {2011}
}
@inproceedings{Bakhoda2009,
abstract = {Modern Graphic Processing Units (GPUs) provide sufficiently flexible programming models that understanding their performance can provide insight in designing tomorrow's manycore processors, whether those are GPUs or otherwise. The combination of multiple, multithreaded, SIMD cores makes studying these GPUs useful in understanding tradeoffs among memory, data, and thread level parallelism. While modern GPUs offer orders of magnitude more raw computing power than contemporary CPUs, many important applications, even those with abundant data level parallelism, do not achieve peak performance. This paper characterizes several non-graphics applications written in NVIDIA's CUDA programming model by running them on a novel detailed microarchitecture performance simulator that runs NVIDIA's parallel thread execution (PTX) virtual instruction set. For this study, we selected twelve non-trivial CUDA applications demonstrating varying levels of performance improvement on GPU hardware (versus a CPU-only sequential version of the application). We study the performance of these applications on our GPU performance simulator with configurations comparable to contemporary high-end graphics cards. We characterize the performance impact of several microarchitecture design choices including choice of interconnect topology, use of caches, design of memory controller, parallel workload distribution mechanisms, and memory request coalescing hardware. Two observations we make are (1) that for the applications we study, performance is more sensitive to interconnect bisection bandwidth rather than latency, and (2) that, for some applications, running fewer threads concurrently than on-chip resources might otherwise allow can improve performance by reducing contention in the memory system. {\textcopyright} 2009 IEEE.},
author = {Bakhoda, Ali and Yuan, George L and Fung, Wilson W.L. and Wong, Henry and Aamodt, Tor M},
booktitle = {ISPASS 2009 - International Symposium on Performance Analysis of Systems and Software},
doi = {10.1109/ISPASS.2009.4919648},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakhoda et al. - Unknown - Analyzing CUDA Workloads Using a Detailed GPU Simulator.pdf:pdf},
isbn = {9781424441846},
mendeley-groups = {Master Project/Selected},
pages = {163--174},
title = {{Analyzing CUDA workloads using a detailed GPU simulator}},
url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2017/02/gpgpusim.ispass09-2.pdf},
year = {2009}
}
@inproceedings{Komoda2013,
abstract = {Future computer systems are built under much stringent power budget due to the limitation of power delivery and cooling systems. To this end, sophisticated power management techniques are required. Power capping is a technique to limit the power consumption of a system to the predetermined level, and has been extensively studied in homogeneous systems. However, few studies about the power capping of CPU-GPU heterogeneous systems have been done yet. In this paper, we propose an efficient power capping technique through coordinating DVFS and task mapping in a single computing node equipped with GPUs. In CPU-GPU heterogeneous systems, settings of the device frequencies have to be considered with task mapping between the CPUs and the GPUs because the frequency scaling can incurs load imbalance between them. To guide the settings of DVFS and task mapping for avoiding power violation and the load imbalance, we develop new empirical models of the performance and the maximum power consumption of a CPU-GPU heterogeneous system. The models enable us to set near-optimal settings of the device frequencies and the task mapping in advance of the application execution. We evaluate the proposed technique with five data-parallel applications on a machine equipped with a single CPU and a single GPU. The experimental result shows that the performance achieved by the proposed power capping technique is comparable to the ideal one. {\textcopyright} 2013 IEEE.},
author = {Komoda, Toshiya and Hayashi, Shingo and Nakada, Takashi and Miwa, Shinobu and Nakamura, Hiroshi},
booktitle = {2013 IEEE 31st International Conference on Computer Design, ICCD 2013},
doi = {10.1109/ICCD.2013.6657064},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Komoda et al. - Unknown - Power Capping of CPU-GPU Heterogeneous Systems through Coordinating DVFS and Task Mapping.pdf:pdf},
isbn = {9781479929870},
keywords = {DVFS,GPGPU,Power Capping,Task Mapping},
mendeley-groups = {Master Project/Selected},
pages = {349--356},
title = {{Power capping of CPU-GPU heterogeneous systems through coordinating DVFS and task mapping}},
url = {http://www.cse.chalmers.se/{~}sica/phd/mappingstudy/primarystudies/S114.pdf},
year = {2013}
}
@inproceedings{Jiao2010,
abstract = {Nowadays Graphic Processing Units (GPU) are gaining increasing popularity in high performance computing (HPC). While modern GPUs can offer much more computational power than CPUs, they also consume much more power. Energy efficiency is one of the most important factors that will affect a broader adoption of GPUs in HPC. In this paper, we systematically characterize the power and energy efficiency of GPU computing. Specifically, using three different applications with various degrees of compute and memory intensiveness, we investigate the correlation between power consumption and different computational patterns under various voltage and frequency levels. Our study revealed that energy saving mechanisms on GPUs behave considerably different than CPUs. The characterization results also suggest possible ways to improve the "greenness" of GPU computing. {\textcopyright} 2010 IEEE.},
author = {Jiao, Y and Lin, H and Balaji, P and Feng, W},
booktitle = {Proceedings - 2010 IEEE/ACM International Conference on Green Computing and Communications, GreenCom 2010, 2010 IEEE/ACM International Conference on Cyber, Physical and Social Computing, CPSCom 2010},
doi = {10.1109/GreenCom-CPSCom.2010.143},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiao et al. - Unknown - Power and Performance Characterization of Computational Kernels on the GPU.pdf:pdf},
isbn = {9780769543314},
mendeley-groups = {Master Project/Selected},
pages = {221--228},
title = {{Power and performance characterization of computational kernels on the GPU}},
url = {https://research.cs.vt.edu/synergy/pubs/papers/yang-perf-power-GPU-DVFS-greencom10.pdf},
year = {2010}
}
@article{Ma2009,
abstract = {In recent years, more and more transistors have been integrated within the GPU, which has resulted in steadily rising power consumption requirements. In this paper we present a preliminary scheme to statistically analyze and model the power consumption of amainstream GPU (NVidia GeForce 8800gt) by exploiting the innate coupling among power consumption characteristics, runtime performance, and dynamic workloads. Based on the recorded run-time GPU workload signals, our trained statistical model is capable of robustly and accurately predict- ing power consumption of the target GPU. To the best of our knowledge, this study is the first work that applies statistical analysis to model the power consumption of a mainstream GPU, and its results provide useful insights for future endeavors of building energy-efficient GPU computing paradigms.},
author = {Ma, Xiaohan and Zhong, Lin},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ma, Zhong - 2009 - Statistical Power Consumption Analysis and Modeling for GPU-based Computing.pdf:pdf},
journal = {Proceedings of the SOSP Workshop on Power Aware Computing and Systems (HotPower '09)},
mendeley-groups = {Master Project/Selected},
pages = {None},
title = {{Statistical Power Consumption Analysis and Modeling for GPU-based Computing}},
url = {https://www.yecl.org/publications/ma09hotpower.pdf http://www.sigops.org/sosp/sosp09/hotpower.html},
year = {2009}
}
@inproceedings{Markidis,
abstract = {The NVIDIA Volta GPU microarchitecture introduces a specialized unit, called Tensor Core that performs one matrix-multiply-and-accumulate on 4x4 matrices per clock cycle. The NVIDIA Tesla V100 accelerator, featuring the Volta microarchitecture, provides 640 Tensor Cores with a theoretical peak performance of 125 Tflops/s in mixed precision. In this paper, we investigate current approaches to program NVIDIA Tensor Cores, their performances and the precision loss due to computation in mixed precision. Currently, NVIDIA provides three different ways of programming matrix-multiply-and-accumulate on Tensor Cores: the CUDA Warp Matrix Multiply Accumulate (WMMA) API, CUTLASS, a templated library based on WMMA, and cuBLAS GEMM. After experimenting with different approaches, we found that NVIDIA Tensor Cores can deliver up to 83 Tflops/s in mixed precision on a Tesla V100 GPU, seven and three times the performance in single and half precision respectively. A WMMA implementation of batched GEMM reaches a performance of 4 Tflops/s. While precision loss due to matrix multiplication with half precision input might be critical in many HPC applications, it can be considerably reduced at the cost of increased computation. Our results indicate that HPC applications using matrix multiplications can strongly benefit from using of NVIDIA Tensor Cores.},
archivePrefix = {arXiv},
arxivId = {1803.04014},
author = {Markidis, Stefano and Chien, Steven Wei Der and Laure, Erwin and Peng, Ivy Bo and Vetter, Jeffrey S.},
booktitle = {Proceedings - 2018 IEEE 32nd International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2018},
doi = {10.1109/IPDPSW.2018.00091},
eprint = {1803.04014},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Markidis et al. - 2018 - NVIDIA tensor core programmability, performance {\&} precision.pdf:pdf},
isbn = {9781538655559},
keywords = {GEMM,GPU Programming,Mixed Precision,NVIDIA Tensor Cores},
mendeley-groups = {Literature Study and Seminar/Selected,Literature Study and Seminar,Master Project/Unread},
pages = {522--531},
title = {{NVIDIA tensor core programmability, performance {\&} precision}},
url = {https://arxiv.org/abs/1803.04014},
year = {2018}
}
@inproceedings{Haidar2018a,
abstract = {As parallel computers approach exascale, power efficiency in high-performance computing (HPC) systems is of increasing concern. Exploiting both the hardware features and algorithms is an effective solution to achieve power efficiency, and to address the energy constraints in modern and future HPC systems. In this work, we present a novel design and implementation of an energy-efficient solution for dense linear systems of equations, which are at the heart of large-scale HPC applications. The proposed energy-efficient linear system solvers are based on two main components: (1) iterative refinement techniques, and (2) reduced-precision computing features in modern accelerators and coprocessors. While most of the energy efficiency approaches aim to reduce the consumption with a minimal performance penalty, our method improves both the performance and the energy efficiency. Compared to highly-optimized linear system solvers, our kernels deliver the same accuracy solution up to 2× faster and reduce the energy consumption up to half on Intel Knights Landing (KNL) architectures. By efficiently using the Tensor Cores available in the NVIDIA V100 PCIe GPUs, the speedups can be up to 4×, with more than 80{\%} reduction in the energy consumption.},
author = {Haidar, Azzam and Abdelfattah, Ahmad and Zounon, Mawussi and Wu, Panruo and Pranesh, Srikara and Tomov, Stanimire and Dongarra, Jack},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-93698-7_45},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haidar et al. - 2018 - The design of fast and energy-efficient linear solvers On the potential of half-precision arithmetic and iterativ.pdf:pdf},
isbn = {9783319936970},
issn = {16113349},
keywords = {FP16,HPC,Mixed-precision,Solvers,Tensor cores},
mendeley-groups = {Literature Study and Seminar/Selected,Master Project/Unread},
pages = {586--600},
publisher = {https},
title = {{The design of fast and energy-efficient linear solvers: On the potential of half-precision arithmetic and iterative refinement techniques}},
url = {https://doi.org/10.1007/978-3-319-93698-7{\_}45},
volume = {10860 LNCS},
year = {2018}
}
@inproceedings{Haidar2019a,
abstract = {Low-precision floating-point arithmetic is a powerful tool for accelerating scientific computing applications, especially those in artificial intelligence. Here, we present an investigation showing that other high-performance computing (HPC) applications can also harness this power. Specifically, we use the general HPC problem, Ax b, where A is a large dense matrix, and a double precision (FP64) solution is needed for accuracy. Our approach is based on mixed-precision (FP16-FP64) iterative refinement, and we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly tuned implementations. These new methods show how using half-precision Tensor Cores (FP16-TC) for the arithmetic can provide up to 4× speedup. This is due to the performance boost that the FP16-TC provide as well as to the improved accuracy over the classical FP16 arithmetic that is obtained because the GEMM accumulation occurs in FP32 arithmetic.},
author = {Haidar, Azzam and Tomov, Stanimire and Dongarra, Jack and Higham, Nicholas J},
booktitle = {Proceedings - International Conference for High Performance Computing, Networking, Storage, and Analysis, SC 2018},
doi = {10.1109/SC.2018.00050},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haidar et al. - 2019 - Harnessing GPU Tensor cores for fast FP16 arithmetic to speed up mixed-precision iterative refinement solvers.pdf:pdf},
isbn = {9781538683842},
keywords = {FP16 Arithmetic,GPU Computing,Half Precision,Iterative Refinement Computation,Linear Algebra,Mixed Precision Solvers},
mendeley-groups = {Literature Study and Seminar/Selected,Master Project/Unread},
pages = {603--613},
title = {{Harnessing GPU Tensor cores for fast FP16 arithmetic to speed up mixed-precision iterative refinement solvers}},
url = {https://www.netlib.org/utk/people/JackDongarra/PAPERS/haidar{\_}fp16{\_}sc18.pdf},
year = {2019}
}
@inproceedings{Bombieri2016,
abstract = {GPU-Accelerated applications are becoming increasingly common in high-performance computing as well as in low-power heterogeneous embedded systems. Nevertheless, GPU programming is a challenging task, especially if a GPU application has to be tuned to fully take advantage of the GPU architectural configuration. Even more challenging is the application tuning by considering power and energy consumption, which have emerged as first-order design constraints in addition to performance. Solving bottlenecks of a GPU application such as high thread divergence or poor memory coalescing have a different impact on the overall performance, power and energy consumption. Such an impact also depends on the GPU device on which the application is run. This paper presents a suite of microbenchmarks, which are specialized chunks of GPU code that exercise specific device components (e.g., arithmetic instruction units, shared memory, cache, DRAM, etc.) and that provide the actual characteristics of such components in terms of throughput, power, and energy consumption. The suite aims at enriching standard profiler information and guiding the GPU application tuning on a specific GPU architecture by considering all three design constraints (i.e., power, performance, energy consumption). The paper presents the results obtained by applying the proposed suite to characterize two different GPU devices and to understand how application tuning may impact differently on them.},
author = {Bombieri, Nicola and Busato, Federico and Fummi, Franco and Scala, Michele},
booktitle = {2016 11th IEEE International Symposium on Industrial Embedded Systems, SIES 2016 - Proceedings},
doi = {10.1109/SIES.2016.7509423},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bombieri et al. - Unknown - MIPP A Microbenchmark Suite for Performance, Power, and Energy Consumption Characterization of GPU architect.pdf:pdf},
isbn = {9781509022823},
mendeley-groups = {Master Project/Related},
title = {{MIPP: A microbenchmark suite for performance, power, and energy consumption characterization of GPU architectures}},
year = {2016}
}
@article{Fahad2019,
abstract = {Energy of computing is a serious environmental concern and mitigating it is an important technological challenge. Accurate measurement of energy consumption during an application execution is key to application-level energy minimization techniques. There are three popular approaches to providing it: (a) System-level physical measurements using external power meters; (b) Measurements using on-chip power sensors and (c) Energy predictive models. In this work, we present a comprehensive study comparing the accuracy of state-of-the-art on-chip power sensors and energy predictive models against system-level physical measurements using external power meters, which we consider to be the ground truth. We show that the average error of the dynamic energy profiles obtained using on-chip power sensors can be as high as 73{\%} and the maximum reaches 300{\%} for two scientific applications, matrix-matrix multiplication and 2D fast Fourier transform for a wide range of problem sizes. The applications are executed on three modern Intel multicore CPUs, two Nvidia GPUs and an Intel Xeon Phi accelerator. The average error of the energy predictive models employing performance monitoring counters (PMCs) as predictor variables can be as high as 32{\%} and the maximum reaches 100{\%} for a diverse set of seventeen benchmarks executed on two Intel multicore CPUs (one Haswell and the other Skylake). We also demonstrate that using inaccurate energy measurements provided by on-chip sensors for dynamic energy optimization can result in significant energy losses up to 84{\%}. We show that, owing to the nature of the deviations of the energy measurements provided by on-chip sensors from the ground truth, calibration can not improve the accuracy of the on-chip sensors to an extent that can allow them to be used in optimization of applications for dynamic energy. Finally, we present the lessons learned, our recommendations for the use of on-chip sensors and energy predictive models and future directions.},
author = {Fahad, Muhammad and Shahid, Arsalan and Manumachu, Ravi Reddy and Lastovetsky, Alexey},
doi = {10.3390/en12112204},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fahad et al. - 2019 - A comparative study of methods for measurement of energy of computing.pdf:pdf},
issn = {19961073},
journal = {Energies},
keywords = {Energy efficiency,Energy predictive models,GPU,Multicore CPU,NVML,Performance monitoring counters,Power aensors,Power meters,RAPL,Xeon Phi},
mendeley-groups = {Master Project/Related},
month = {jun},
number = {11},
publisher = {MDPI AG},
title = {{A comparative study of methods for measurement of energy of computing}},
volume = {12},
year = {2019}
}
@techreport{Wagt2020,
author = {Wagt, Julius},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wagt - 2020 - Energy Efficiency for Heterogeneous Computing.pdf:pdf},
mendeley-groups = {Master Project/Related},
title = {{Energy Efficiency for Heterogeneous Computing}},
year = {2020}
}
@techreport{Ehsan2019,
author = {Ehsan, : and Esfahani, Sharifi and Sclocco, Alessio and Varbanescu, Ana},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ehsan et al. - 2019 - Auto-Tuning for Energy Efficiency on GPUs Computer Science-Parallel Computing Systems.pdf:pdf},
mendeley-groups = {Master Project/Related},
title = {{Auto-Tuning for Energy Efficiency on GPUs Computer Science-Parallel Computing Systems}},
year = {2019}
}
@article{Mei2017,
abstract = {Energy efficiency has become one of the top design criteria for current computing systems. The dynamic voltage and frequency scaling (DVFS) has been widely adopted by laptop computers, servers, and mobile devices to conserve energy, while the GPU DVFS is still at a certain early age. This paper aims at exploring the impact of GPU DVFS on the application performance and power consumption, and furthermore, on energy conservation. We survey the state-of-the-art GPU DVFS characterizations, and then summarize recent research works on GPU power and performance models. We also conduct real GPU DVFS experiments on NVIDIA Fermi and Maxwell GPUs. According to our experimental results, GPU DVFS has significant potential for energy saving. The effect of scaling core voltage/frequency and memory voltage/frequency depends on not only the GPU architectures, but also the characteristic of GPU applications.},
archivePrefix = {arXiv},
arxivId = {1610.01784},
author = {Mei, Xinxin and Wang, Qiang and Chu, Xiaowen},
doi = {10.1016/j.dcan.2016.10.001},
eprint = {1610.01784},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mei, Wang, Chu - 2017 - A survey and measurement study of GPU DVFS on energy conservation.pdf:pdf},
issn = {23528648},
journal = {Digital Communications and Networks},
keywords = {Dynamic voltage and frequency scaling,Energy efficiency,Graphics processing unit},
mendeley-groups = {Master Project/Unread},
month = {may},
number = {2},
pages = {89--100},
publisher = {Chongqing University of Posts and Telecommunications},
title = {{A survey and measurement study of GPU DVFS on energy conservation}},
volume = {3},
year = {2017}
}
@inproceedings{Mei2013,
abstract = {Nowadays, GPUs are widely used to accelerate many high performance computing applications. Energy conservation of such computing systems has become an important research topic. Dynamic voltage/frequency scaling (DVFS) is proved to be an appealing method for saving energy for traditional computing centers. However, there is still a lack of firsthand study on the effectiveness of GPU DVFS. This paper presents a thorough measurement study that aims to explore how GPU DVFS affects the system energy consumption. We conduct experiments on a real GPU platform with 37 benchmark applications. Our results show that GPU voltage/frequency scaling is an effective approach to conserving energy. For example, by scaling down the GPU core voltage and frequency, we have achieved an average of 19.28{\%} energy reduction compared with the default setting, while giving up no more than 4{\%} of performance. For all tested GPU applications, core voltage scaling is significantly effective to reduce system energy consumption. Meanwhile the effects of scaling core frequency and memory frequency depend on the characteristics of GPU applications. {\textcopyright} 2013 ACM.},
author = {Mei, Xinxin and Yung, Ling Sing and Zhao, Kaiyong and Chu, Xiaowen},
booktitle = {Proceedings of the Workshop on Power-Aware Computing and Systems, HotPower 2013},
doi = {10.1145/2525526.2525852},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mei et al. - 2013 - A measurement study of GPU DVFS on energy conservation.pdf:pdf},
isbn = {9781450324588},
keywords = {GPU,energy conservation,voltage/frequency scaling},
mendeley-groups = {Master Project/Unread},
title = {{A measurement study of GPU DVFS on energy conservation}},
url = {https://www.researchgate.net/publication/262365062},
year = {2013}
}
@inproceedings{Martineau2019a,
abstract = {The V100 GPU is the newest server-grade GPU produced by NVIDIA and introduces a number of new hardware and API features. This paper details the results of benchmarking the V100 GPU and demonstrates that it is a significant generational improvement, increasing memory bandwidth, cache bandwidth, and reducing latency. A major new addition is the Tensor core units, which have been marketed as deep learning acceleration features that enable the computation of a 4 × 4 × 4 half precision matrix-multiply-accumulate operation in a single clock cycle. This paper confirms that the Tensor cores offer considerable performance gains for half precision general matrix multiplication; however, programming them requires fine control of the memory hierarchy that is typically unnecessary for other applications.},
author = {Martineau, Matt and Atkinson, Patrick and McIntosh-Smith, Simon},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-10549-5_35},
file = {:home/qub1-creation/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Martineau, Atkinson, McIntosh-Smith - 2019 - Benchmarking the NVIDIA V100 GPU and tensor cores.pdf:pdf},
isbn = {9783030105488},
issn = {16113349},
mendeley-groups = {Literature Study and Seminar/Selected,Master Project/Unread},
month = {aug},
pages = {444--455},
publisher = {Springer Verlag},
title = {{Benchmarking the NVIDIA V100 GPU and tensor cores}},
volume = {11339 LNCS},
year = {2019}
}
